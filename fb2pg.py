#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fbddl2pg_plus.py — Firebird/InterBase DDL → PostgreSQL DDL (best effort, com heurística de autoincrement)

Uso:
  python fbddl2pg_plus.py input.sql [-o out.pg.sql] [--computed generated|comment] [--verbose]

Principais recursos:
  - Normaliza SET TERM/terminadores
  - RECREATE/CREATE OR ALTER → equivalentes PG
  - GENERATOR/SEQUENCE, SET GENERATOR, GEN_ID → nextval()/setval()
  - Tipos: BLOB SUB_TYPE 1 → TEXT, BLOB → BYTEA; remove CHARACTER SET, COLLATE, SEGMENT SIZE
  - Detecta trigger BEFORE INSERT que faz NEW.col = GEN_ID(seq,1) → converte p/ IDENTITY
    * Emite ALTER TABLE ... ALTER COLUMN ... ADD GENERATED BY DEFAULT AS IDENTITY
    * Comenta a trigger original
  - Computed columns: por padrão GENERATED ALWAYS AS (...) STORED (ou comenta com --computed comment)
  - Comenta PROCEDURE/FUNCTION/EXCEPTION (sem conversão automática)
"""

from pathlib import Path
import argparse
import re
import sys
from typing import Dict, List, Tuple

def log(msg: str, verbose: bool):
    if verbose:
        print(f"[fbddl2pg+] {msg}")

def read_file(p: Path) -> str:
    return p.read_text(encoding="utf-8", errors="ignore")

def write_file(p: Path, s: str):
    p.write_text(s, encoding="utf-8")

def normalize_terminators(sql: str, verbose: bool) -> str:
    t = sql.replace("\r\n", "\n").replace("\r", "\n")
    # Detect custom SET TERM X ;
    # Replace all occurrences of X at statement ends with ';' (simple approach)
    m = re.search(r"(?im)^\s*SET\s+TERM\s+(.+?)\s*;\s*$", t)
    if m:
        term = m.group(1).strip()
        if term and term != ";":
            t = t.replace(term, ";")
            log(f"Substituído terminador '{term}' por ';'.", verbose)
    # Remove all SET TERM lines/blocks
    t = re.sub(r"(?is)^\s*SET\s+TERM\b.*?;$", "", t, flags=re.MULTILINE)
    # Collapse duplicated ';;'
    t = re.sub(r";\s*;", ";", t)
    return t

def convert_recreate_and_create_or_alter(sql: str) -> str:
    def repl(kind: str):
        def _inner(m):
            name = m.group(1)
            return f"DROP {kind.upper()} IF EXISTS {name};\nCREATE {kind.upper()} {name}"
        return _inner
    t = sql
    t = re.sub(r"(?i)\bRECREATE\s+TABLE\s+([A-Za-z_][\w$\.]*)", repl("TABLE"), t)
    t = re.sub(r"(?i)\bRECREATE\s+VIEW\s+([A-Za-z_][\w$\.]*)", repl("VIEW"), t)
    t = re.sub(r"(?i)\bRECREATE\s+(?:SEQUENCE|GENERATOR)\s+([A-Za-z_][\w$\.]*)", repl("SEQUENCE"), t)
    # CREATE OR ALTER VIEW -> CREATE OR REPLACE VIEW
    t = re.sub(r"(?i)\bCREATE\s+OR\s+ALTER\s+VIEW\b", "CREATE OR REPLACE VIEW", t)
    # CREATE OR ALTER TRIGGER: deixaremos para a rotina de triggers (será comentada)
    return t

def convert_sequences(sql: str) -> str:
    t = sql
    t = re.sub(r"(?i)\bCREATE\s+GENERATOR\b", "CREATE SEQUENCE", t)
    # SET GENERATOR seq TO N;
    t = re.sub(r"(?i)\bSET\s+GENERATOR\s+([A-Za-z_][\w$]*)\s+TO\s+(-?\d+)\s*;", r"SELECT setval('\1', \2);", t)
    # GEN_ID(seq,1) -> nextval('seq')
    t = re.sub(r"(?i)\bGEN_ID\(\s*([A-Za-z_][\w$]*)\s*,\s*1\s*\)", r"nextval('\1')", t)
    return t

def convert_types(sql: str) -> str:
    t = sql
    # Remove comandos específicos do Firebird que não existem no PostgreSQL
    t = re.sub(r"(?mi)^\s*SET\s+SQL\s+DIALECT\s+\d+\s*;\s*$", "-- SET SQL DIALECT removido (específico do Firebird)", t)
    t = re.sub(r"(?mi)^\s*SET\s+NAMES\s+\w+\s*;\s*$", "-- SET NAMES removido (use SET client_encoding no PostgreSQL)", t)
    t = re.sub(r"(?mi)^\s*SET\s+CLIENTLIB\s+.*?;\s*$", "-- SET CLIENTLIB removido (específico do Firebird)", t)
    
    # Comentar CREATE DATABASE - versão simplificada
    t = re.sub(r"(?is)CREATE\s+DATABASE\s+[^;]+;", "-- CREATE DATABASE removido (criar manualmente no PostgreSQL)", t)
    
    # Firebird blob segment size
    t = re.sub(r"(?i)\bSEGMENT\s+SIZE\s+\d+\b", "", t)
    # BLOB subtypes
    t = re.sub(r"(?i)\bBLOB\s+SUB_TYPE\s+1\b", "TEXT", t)
    t = re.sub(r"(?i)\bBLOB\s+SUB_TYPE\s+0\b", "BYTEA", t)
    t = re.sub(r"(?i)\bBLOB\b", "BYTEA", t)
    
    # Remove CHARACTER SET / COLLATE
    t = re.sub(r"(?i)\s+CHARACTER\s+SET\s+\w+", "", t)
    t = re.sub(r"(?i)\s+COLLATE\s+\w+", "", t)
    
    # Global temporary table
    t = re.sub(r"(?i)\bCREATE\s+GLOBAL\s+TEMPORARY\s+TABLE\b", "CREATE TEMPORARY TABLE", t)
    
    # DEFAULT 'NOW' -> now()
    t = re.sub(r"(?i)DEFAULT\s+'NOW'\s*", "DEFAULT now()", t)
    
    # Remove elementos específicos do Firebird que não fazem sentido no PostgreSQL
    # Remove EXTERNAL FILE
    t = re.sub(r"(?i)\s+EXTERNAL\s+FILE\s+'[^']*'", "", t)
    
    # Remove PLAN clauses
    t = re.sub(r"(?i)\s+PLAN\s+\([^)]*\)", "", t)
    
    # Remove POSITION na definição de campos
    t = re.sub(r"(?i)\s+POSITION\s+\d+", "", t)
    
    # Remove PAGE_SIZE de database
    t = re.sub(r"(?i)PAGE_SIZE\s*=\s*\d+", "", t)
    
    # Remove LENGTH para CHAR/VARCHAR (será inferido)
    t = re.sub(r"(?i)\s+LENGTH\s+\d+", "", t)
    
    # Converte DOUBLE PRECISION para DOUBLE PRECISION (já compatível)
    # Converte INTEGER para INTEGER (já compatível)  
    # Converte SMALLINT para SMALLINT (já compatível)
    # Converte BIGINT para BIGINT (já compatível)
    
    # Remove SUB_TYPE para outros tipos
    t = re.sub(r"(?i)\s+SUB_TYPE\s+\d+", "", t)
    
    # Remove SCALE específico do Firebird
    t = re.sub(r"(?i)\s+SCALE\s+-?\d+", "", t)
    
    return t

def convert_indexes(sql: str, verbose: bool) -> str:
    """
    Converte índices específicos do Firebird para PostgreSQL
    """
    t = sql
    
    # Remove DESCENDING keyword específico do Firebird (PostgreSQL usa DESC)
    t = re.sub(r"(?i)\bDESCENDING\b", "DESC", t)
    
    # Remove ASCENDING keyword (é o padrão no PostgreSQL)
    t = re.sub(r"(?i)\bASCENDING\b", "", t)
    
    # Converter CREATE INDEX com COMPUTED BY
    computed_idx_re = re.compile(
        r"(?is)CREATE\s+(?:UNIQUE\s+)?INDEX\s+([A-Za-z_][\w$]*)\s+ON\s+([A-Za-z_][\w$\.]*)\s+COMPUTED\s+BY\s*\(([^)]+)\)",
        re.IGNORECASE
    )
    
    def repl_computed_index(m):
        unique = "UNIQUE " if "UNIQUE" in m.group(0).upper() else ""
        index_name = m.group(1)
        table_name = m.group(2)
        expression = m.group(3).strip()
        
        return f"CREATE {unique}INDEX {index_name} ON {table_name} (({expression}))"
    
    t = re.sub(computed_idx_re, repl_computed_index, t)
    
    # Remove PLAN hints do Firebird
    t = re.sub(r"(?i)PLAN\s*\([^)]*\)", "", t)
    
    # Converter índices com STATISTICS
    t = re.sub(r"(?i)\s+STATISTICS\s+\d+", "", t)
    
    if verbose:
        log("Processamento de índices concluído", verbose)
    
    return t

def apply_primary_key_sequences(sql: str, pk_list: List[Tuple[str,str]], verbose: bool) -> str:
    """
    Adiciona sequences para chaves primárias detectadas
    """
    if not pk_list:
        return sql
        
    sequences = ["\n-- Sequences para chaves primárias"]
    seen = set()
    
    for table, col in pk_list:
        key = (table, col)
        if key in seen:
            continue
        seen.add(key)
        
        seq_name = f"{table.lower()}_{col.lower()}_seq"
        sequences.append(f"CREATE SEQUENCE {seq_name};")
        sequences.append(f"ALTER TABLE {table} ALTER COLUMN {col} SET DEFAULT nextval('{seq_name}');")
        sequences.append(f"ALTER SEQUENCE {seq_name} OWNED BY {table}.{col};")
        
        if verbose:
            log(f"Sequence criada para {table}.{col}: {seq_name}", verbose)
    
    return sql + "\n" + "\n".join(sequences) + "\n"

def convert_domains(sql: str, domain_strategy: str, verbose: bool) -> str:
    """
    Converte definições de DOMAIN do Firebird para PostgreSQL
    IMPORTANTE: Esta função deve ser chamada ANTES de convert_types para preservar nomes dos domains
    
    Args:
        domain_strategy: 'keep' para manter domains, 'replace' para substituir por tipos diretos
    """
    # Padrão para capturar CREATE DOMAIN (pode estar em múltiplas linhas)
    domain_re = re.compile(
        r"(?is)CREATE\s+DOMAIN\s+([A-Za-z_][\w$]*)\s+AS\s+([^;]+);",
        re.DOTALL
    )
    
    domains_map = {}
    
    def process_domain(m):
        original_domain_name = m.group(1)  # Preservar o nome original
        domain_def = m.group(2).strip()
        
        # Extrair o tipo base (remover quebras de linha e espaços extras)
        raw_type = re.sub(r'\s+', ' ', domain_def).strip()
        
        # Converter tipos Firebird para PostgreSQL ANTES de processar
        converted_type = raw_type
        # BLOB subtypes
        converted_type = re.sub(r"(?i)\bBLOB\s+SUB_TYPE\s+1\b", "TEXT", converted_type)
        converted_type = re.sub(r"(?i)\bBLOB\s+SUB_TYPE\s+0\b", "BYTEA", converted_type)
        converted_type = re.sub(r"(?i)\bBLOB\b", "BYTEA", converted_type)
        
        # Remove CHARACTER SET / COLLATE
        converted_type = re.sub(r"(?i)\s+CHARACTER\s+SET\s+\w+", "", converted_type)
        converted_type = re.sub(r"(?i)\s+COLLATE\s+\w+", "", converted_type)
        
        # Extrair constraints CHECK se existirem
        check_match = re.search(r"(?i)CHECK\s*\(([^)]+)\)", converted_type)
        check_constraint = check_match.group(1) if check_match else None
        
        # Extrair DEFAULT se existir
        default_match = re.search(r"(?i)DEFAULT\s+([^,\s]+)", converted_type)
        default_value = default_match.group(1) if default_match else None
        
        # Extrair NOT NULL se existir
        not_null = bool(re.search(r"(?i)\bNOT\s+NULL\b", converted_type))
        
        # Limpar o tipo base removendo CHECK, DEFAULT, NOT NULL
        clean_type = re.sub(r"(?i)\s+CHECK\s*\([^)]+\)", "", converted_type)
        clean_type = re.sub(r"(?i)\s+DEFAULT\s+[^,\s]+", "", clean_type)
        clean_type = re.sub(r"(?i)\s+NOT\s+NULL", "", clean_type)
        clean_type = re.sub(r'\s+', ' ', clean_type).strip()
        
        domains_map[original_domain_name] = {
            'type': clean_type,
            'check': check_constraint,
            'default': default_value,
            'not_null': not_null
        }
        
        if verbose:
            log(f"Domain {original_domain_name} mapeado para tipo {clean_type}", verbose)
        
        if domain_strategy == 'keep':
            # Manter o domain convertido para PostgreSQL
            pg_domain = f"CREATE DOMAIN {original_domain_name} AS {clean_type}"
            
            if default_value:
                pg_domain += f" DEFAULT {default_value}"
            if not_null:
                pg_domain += " NOT NULL"
            if check_constraint:
                pg_domain += f" CHECK ({check_constraint})"
            
            pg_domain += ";"
            
            return f"-- Domain convertido do Firebird\n{pg_domain}"
        else:
            # Comentar a definição original do domain completamente
            lines = m.group(0).split('\n')
            commented_lines = [f"-- {line}" if line.strip() else line for line in lines]
            return f"-- TODO: DOMAIN {original_domain_name} convertido para uso direto do tipo {clean_type}\n" + "\n".join(commented_lines)
    
    # Processar domains
    t = re.sub(domain_re, process_domain, sql)
    
    if domain_strategy == 'replace':
        # Substituir uso de domains por tipos diretos
        for domain_name, domain_info in domains_map.items():
            # Substituir referências ao domain pelo tipo base
            domain_usage_re = re.compile(rf"\b{re.escape(domain_name)}\b", re.IGNORECASE)
            replacement = domain_info['type']
            
            # Adicionar NOT NULL se necessário
            if domain_info['not_null']:
                replacement += " NOT NULL"
                
            # Adicionar DEFAULT se necessário  
            if domain_info['default']:
                replacement += f" DEFAULT {domain_info['default']}"
                
            t = re.sub(domain_usage_re, replacement, t)
            
            # Se houver CHECK constraint, adicionar como comentário para revisão manual
            if domain_info['check']:
                check_comment = f"-- TODO: Revisar CHECK constraint do domain {domain_name}: CHECK ({domain_info['check']})\n"
                t = check_comment + t
    
    return t

def convert_computed_columns(sql: str, strategy: str, verbose: bool) -> str:
    # Detect "<col> <type> COMPUTED BY (<expr>)"
    pattern = re.compile(r"(\b[A-Za-z_][\w$]*\b)\s+([A-Za-z][\w\s\(\),]*)\s+COMPUTED\s+BY\s*\((.*?)\)", re.IGNORECASE | re.DOTALL)
    def repl_generated(m):
        col, typ, expr = m.group(1), m.group(2), m.group(3).strip()
        if re.search(r"\b(NEW|OLD)\.", expr, re.I):
            return f"{col} {typ} /* TODO: Firebird COMPUTED BY ({expr}) - revisar */"
        return f"{col} {typ} GENERATED ALWAYS AS ({expr}) STORED"
    def repl_comment(m):
        col, typ, expr = m.group(1), m.group(2), m.group(3).strip()
        return f"{col} {typ} /* TODO: Firebird COMPUTED BY ({expr}) - revisar no PostgreSQL */"
    if strategy == "generated":
        return re.sub(pattern, repl_generated, sql)
    return re.sub(pattern, repl_comment, sql)

def detect_primary_keys(sql: str, verbose: bool) -> Tuple[str, List[Tuple[str,str]]]:
    """
    Detecta chaves primárias em tabelas e gera sequences para elas se forem numéricas
    Retorna sql modificado e lista de (table, column) para criar sequences
    """
    results: List[Tuple[str,str]] = []
    
    # Regex para capturar definições de tabela completas
    table_re = re.compile(
        r"(?is)CREATE\s+TABLE\s+([A-Za-z_][\w$\.]*)\s*\(\s*(.*?)\s*\)\s*;",
        re.DOTALL
    )
    
    # Regex para capturar ALTER TABLE ... ADD PRIMARY KEY
    alter_pk_re = re.compile(
        r"(?i)ALTER\s+TABLE\s+([A-Za-z_][\w$\.]*)\s+ADD\s+PRIMARY\s+KEY\s*\(\s*([A-Za-z_][\w$]*)\s*\)\s*;",
        re.IGNORECASE
    )
    
    # Coletar informações sobre colunas das tabelas
    table_columns = {}
    
    def process_table(m):
        table_name = m.group(1)
        table_body = m.group(2)
        
        # Extrair definições de colunas
        col_definitions = []
        for line in table_body.split('\n'):
            line = line.strip()
            if line and not line.startswith('--'):
                # Procurar por definições de coluna (nome tipo ...)
                col_match = re.match(r"([A-Za-z_][\w$]*)\s+(INTEGER|BIGINT|SMALLINT|NUMERIC|DECIMAL)", line, re.IGNORECASE)
                if col_match:
                    col_name = col_match.group(1)
                    col_type = col_match.group(2).upper()
                    col_definitions.append((col_name, col_type))
        
        table_columns[table_name] = col_definitions
        
        # Procurar por PRIMARY KEY constraint inline na tabela
        pk_inline_re = re.compile(r"(?i)([A-Za-z_][\w$]*)\s+(?:INTEGER|BIGINT|SMALLINT|NUMERIC|DECIMAL)\s+(?:NOT\s+NULL\s+)?PRIMARY\s+KEY", re.IGNORECASE)
        pk_constraint_re = re.compile(r"(?i)(?:CONSTRAINT\s+[A-Za-z_][\w$]*\s+)?PRIMARY\s+KEY\s*\(\s*([A-Za-z_][\w$]*)\s*\)", re.IGNORECASE)
        
        # Verificar PRIMARY KEY inline
        pk_inline_match = pk_inline_re.search(table_body)
        if pk_inline_match:
            column = pk_inline_match.group(1)
            results.append((table_name, column))
            if verbose:
                log(f"Detectada PK inline: {table_name}.{column}", verbose)
        
        # Verificar PRIMARY KEY como constraint separada
        pk_constraint_match = pk_constraint_re.search(table_body)
        if pk_constraint_match and not pk_inline_match:
            column = pk_constraint_match.group(1)
            # Verificar se a coluna é numérica
            for col_name, col_type in col_definitions:
                if col_name.upper() == column.upper() and col_type in ['INTEGER', 'BIGINT', 'SMALLINT', 'NUMERIC', 'DECIMAL']:
                    results.append((table_name, column))
                    if verbose:
                        log(f"Detectada PK constraint: {table_name}.{column}", verbose)
                    break
        
        return m.group(0)  # Retorna o texto original da tabela
    
    def process_alter_pk(m):
        table_name = m.group(1)
        column = m.group(2)
        
        # Verificar se a coluna é numérica baseado nas definições das tabelas
        if table_name in table_columns:
            for col_name, col_type in table_columns[table_name]:
                if col_name.upper() == column.upper() and col_type in ['INTEGER', 'BIGINT', 'SMALLINT', 'NUMERIC', 'DECIMAL']:
                    results.append((table_name, column))
                    if verbose:
                        log(f"Detectada PK em ALTER TABLE: {table_name}.{column}", verbose)
                    break
        
        return m.group(0)  # Retorna o ALTER TABLE original
    
    # Processar tabelas primeiro para coletar informações sobre colunas
    new_sql = re.sub(table_re, process_table, sql)
    
    # Processar ALTER TABLE ADD PRIMARY KEY
    new_sql = re.sub(alter_pk_re, process_alter_pk, new_sql)
    
    return new_sql, results

def detect_autoinc_triggers(sql: str, verbose: bool) -> Tuple[str, List[Tuple[str,str,str]]]:
    """
    Identifica triggers BEFORE INSERT que fazem NEW.<col> = GEN_ID(<seq>, 1)
    Retorna sql com essas triggers comentadas e uma lista de (table, column, seq)
    """
    results: List[Tuple[str,str,str]] = []

    # Regex para capturar CREATE [OR ALTER] TRIGGER ... FOR <table> ... BEFORE INSERT ... NEW.<col> = GEN_ID(<seq>, 1)
    trig_re = re.compile(
        r"(?is)\bCREATE\s+(?:OR\s+ALTER\s+)?TRIGGER\s+([A-Za-z_][\w$]*)\s+FOR\s+([A-Za-z_][\w$\.]*)\s.*?\bBEFORE\s+INSERT\b.*?"
        r"(?:NEW\.\s*([A-Za-z_][\w$]*))\s*=\s*GEN_ID\(\s*([A-Za-z_][\w$]*)\s*,\s*1\s*\)\s*;.*?\bEND\s*;"
    )
    def repl(m):
        trig_name = m.group(1)
        table = m.group(2)
        col = m.group(3)
        seq = m.group(4)
        results.append((table, col, seq))
        body = m.group(0)
        return f"/* TODO: Trigger de autoincrement substituída por IDENTITY em {table}.{col} (seq {seq})\n{body}\n*/"
    new_sql = re.sub(trig_re, repl, sql)
    if results and verbose:
        log(f"Detectadas triggers de autoincrement: {results}", verbose)
    return new_sql, results

def comment_remaining_triggers_procs_funcs(sql: str) -> str:
    # Comentar outras triggers
    t = sql
    t = re.sub(r"(?is)\bCREATE\s+(?:OR\s+ALTER\s+)?TRIGGER\b.*?\bEND\s*;", lambda m: f"/* TODO: Converter trigger manualmente:\n{m.group(0)}\n*/", t)
    # Comentar procedures
    t = re.sub(r"(?is)^\s*CREATE\s+(?:OR\s+ALTER\s+)?PROCEDURE\b.*?\bEND\s*;", lambda m: f"/* TODO: Converter PROCEDURE manualmente:\n{m.group(0)}\n*/", t, flags=re.MULTILINE)
    # Comentar functions
    t = re.sub(r"(?is)^\s*CREATE\s+(?:OR\s+ALTER\s+)?FUNCTION\b.*?\bEND\s*;", lambda m: f"/* TODO: Converter FUNCTION manualmente:\n{m.group(0)}\n*/", t, flags=re.MULTILINE)
    # Exceptions
    t = re.sub(r"(?mi)^\s*CREATE\s+EXCEPTION\b.*?;\s*$", "-- TODO: CREATE EXCEPTION (sem equivalente direto no PG)", t)
    return t

def strip_misc(sql: str) -> str:
    t = sql
    # COMMIT isolado
    t = re.sub(r"(?mi)^\s*COMMIT\s*;\s*$", "", t)
    # Múltiplas linhas em branco
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t

def fix_orphan_block_comments(sql: str) -> str:
    """Remove linhas contendo apenas '*/' que não possuem abertura correspondente.

    Causa provável: algum passo de substituição removeu a linha de abertura '/* ...' de um
    bloco de comentário, deixando o fechamento isolado e causando erro de sintaxe no PostgreSQL.
    Em vez de tentar reconstruir o bloco perdido (informação não disponível), apenas removemos
    o fechamento órfão para gerar um script válido.
    """
    lines = sql.splitlines()
    open_count = 0
    out = []
    for line in lines:
        stripped = line.strip()
        # Contagem aproximada de aberturas/fechamentos existentes na linha
        # Ignora casos com citações complexas (não necessário para DDL típico)
        opens = line.count("/*")
        closes = line.count("*/")
        # Se linha possui apenas '*/' (talvez com espaços) e não há bloco aberto, descartar
        if opens == 0 and closes == 1 and stripped == "*/" and open_count == 0:
            # descarta linha órfã
            continue
        out.append(line)
        # Atualiza contagem após processar (para não confundir linha isolada)
        open_count += opens - closes
        if open_count < 0:  # segurança: nunca negativo
            open_count = 0
    return "\n".join(out)

def apply_identity_alterations(sql: str, autoinc_list: List[Tuple[str,str,str]], verbose: bool) -> str:
    if not autoinc_list:
        return sql
    alters = ["\n-- Conversões de autoincrement (triggers ➜ IDENTITY)"]
    seen = set()
    for table, col, seq in autoinc_list:
        key = (table, col)
        if key in seen:
            continue
        seen.add(key)
        alters.append(f"ALTER TABLE {table} ALTER COLUMN {col} ADD GENERATED BY DEFAULT AS IDENTITY;  -- seq anterior: {seq}")
        # Dica opcional para setar valor de sequência original:
        alters.append(f"-- Caso queira manter continuidade, rode depois: SELECT setval(pg_get_serial_sequence('{table}', '{col}'), (SELECT last_value FROM {seq})::bigint, true);")
    return sql + "\n" + "\n".join(alters) + "\n"

def convert(sql: str, computed_strategy: str, domain_strategy: str, verbose: bool) -> str:
    t = normalize_terminators(sql, verbose)
    t = convert_recreate_and_create_or_alter(t)
    t = convert_sequences(t)
    
    # Converter domains ANTES de converter tipos para preservar nomes originais
    t = convert_domains(t, domain_strategy, verbose)
    
    # Agora converter tipos depois que domains já foram processados
    t = convert_types(t)
    t = convert_indexes(t, verbose)
    t = convert_computed_columns(t, computed_strategy, verbose)
    
    # Detect primary keys for sequence generation
    t, pk_list = detect_primary_keys(t, verbose)
    
    # Detect and swap autoincrement triggers first
    t, autoinc = detect_autoinc_triggers(t, verbose)
    
    # Comment remaining triggers/procedures/functions
    t = comment_remaining_triggers_procs_funcs(t)
    
    # Final cleanups
    t = strip_misc(t)
    
    # Apply sequences for primary keys
    t = apply_primary_key_sequences(t, pk_list, verbose)
    
    # Append identity alterations
    t = apply_identity_alterations(t, autoinc, verbose)

    # Corrigir possíveis '*/' órfãos que causem erro de sintaxe
    t = fix_orphan_block_comments(t)
    
    return t

def main():
    ap = argparse.ArgumentParser(
        prog="fbddl2pg_plus",
        description="Converte DDL Firebird para PostgreSQL com heurísticas (inclui autoincrement via IDENTITY).",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    ap.add_argument("input", help="Arquivo .sql de origem (Firebird).")
    ap.add_argument("-o", "--output", help="Arquivo de saída (.pg.sql).", default=None)
    ap.add_argument("--computed", choices=["generated", "comment"], default="generated",
                    help="Como converter colunas COMPUTED BY.")
    ap.add_argument("--domains", choices=["keep", "replace"], default="keep",
                    help="Como tratar domains: 'keep' para manter como domains PostgreSQL, 'replace' para substituir por tipos diretos.")
    ap.add_argument("--verbose", action="store_true", help="Exibir logs detalhados.")
    args = ap.parse_args()

    in_path = Path(args.input)
    if not in_path.exists():
        print(f"Erro: arquivo não encontrado: {in_path}", file=sys.stderr)
        sys.exit(1)
    out_path = Path(args.output) if args.output else Path(str(in_path) + ".pg.sql")

    raw = read_file(in_path)
    converted = convert(raw, args.computed, args.domains, args.verbose)

    header = (
        f"-- Gerado por fbddl2pg_plus.py\n"
        f"-- Origem: {in_path.name}\n"
        f"-- Estratégia COMPUTED: {args.computed}\n"
        f"-- Estratégia DOMAINS: {args.domains}\n\n"
    )
    write_file(out_path, header + converted)
    print(f"✅ Conversão concluída → {out_path}")

if __name__ == "__main__":
    main()